{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b417323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "from typing import Dict, Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "828c5eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        self.height = 5\n",
    "        self.width = 5\n",
    "        self.actions = ['north', 'south', 'east', 'west']\n",
    "        \n",
    "        # special states A and B\n",
    "        self.special_states = {\n",
    "            (0, 1): {'next_state': (4, 1), 'reward': 10},  # A → A′\n",
    "            (0, 3): {'next_state': (2, 3), 'reward': 5}    # B → B′\n",
    "        }\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> Tuple[int,int]:\n",
    "        # random start\n",
    "        self.current_state = (random.randint(0, 4), random.randint(0, 4))\n",
    "        return self.current_state\n",
    "\n",
    "    def get_available_actions(self) -> List[str]:\n",
    "        return self.actions\n",
    "\n",
    "    def step(self, action: str) -> Tuple[Tuple[int,int], float]:\n",
    "        state = self.current_state\n",
    "\n",
    "        # special transition?\n",
    "        if state in self.special_states:\n",
    "            special = self.special_states[state]\n",
    "            self.current_state = special['next_state']\n",
    "            return self.current_state, special['reward']\n",
    "        \n",
    "        # normal move\n",
    "        x, y = state\n",
    "        if action == 'north':\n",
    "            new_state = (x-1, y) if x > 0 else state\n",
    "        elif action == 'south':\n",
    "            new_state = (x+1, y) if x < self.height-1 else state\n",
    "        elif action == 'east':\n",
    "            new_state = (x, y+1) if y < self.width-1 else state\n",
    "        elif action == 'west':\n",
    "            new_state = (x, y-1) if y > 0 else state\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action\")\n",
    "\n",
    "        # hit wall?\n",
    "        reward = -1 if new_state == state else 0\n",
    "        self.current_state = new_state\n",
    "        return new_state, reward\n",
    "\n",
    "    def render(self):\n",
    "        grid = np.full((self.height, self.width), '.')\n",
    "        grid[self.current_state] = 'A'\n",
    "        print(grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c4b1231",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QTable:\n",
    "    def __init__(self, states: List[Tuple[int,int]], actions: List[str]):\n",
    "        self.q_table = {\n",
    "            state: {action: 0.0 for action in actions}\n",
    "            for state in states\n",
    "        }\n",
    "\n",
    "    def get_q_value(self, state: Tuple[int,int], action: str) -> float:\n",
    "        return self.q_table[state][action]\n",
    "\n",
    "    def get_max_q_value(self, state: Tuple[int,int]) -> float:\n",
    "        return max(self.q_table[state].values())\n",
    "\n",
    "    def get_best_action(self, state: Tuple[int,int]) -> str:\n",
    "        max_q = self.get_max_q_value(state)\n",
    "        best_actions = [a for a,q in self.q_table[state].items() if q == max_q]\n",
    "        return random.choice(best_actions)\n",
    "\n",
    "    def update(self,\n",
    "               state: Tuple[int,int],\n",
    "               action: str,\n",
    "               reward: float,\n",
    "               next_state: Tuple[int,int],\n",
    "               alpha: float,\n",
    "               gamma: float):\n",
    "        current_q = self.get_q_value(state, action)\n",
    "        max_next_q = self.get_max_q_value(next_state)\n",
    "        new_q = current_q + alpha * (reward + gamma * max_next_q - current_q)\n",
    "        self.q_table[state][action] = new_q\n",
    "\n",
    "\n",
    "def get_optimal_policy(qtable: QTable) -> Dict[Tuple[int,int], str]:\n",
    "    policy = {}\n",
    "    for state in qtable.q_table:\n",
    "        max_q = qtable.get_max_q_value(state)\n",
    "        best_actions = [a for a,q in qtable.q_table[state].items() if q == max_q]\n",
    "        policy[state] = random.choice(best_actions)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c02b7b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Gridworld...\n",
      "Grid size: 5x5\n",
      "Special_states = {'A': (0,1), 'B': (0,3)}\n",
      "Next_to_states = {\"A'\": (4,1), \"B'\": (2,3)}\n",
      "Special_rewards = {'A': 10, 'B': 5}\n",
      "Starting Q-learning with parameters:\n",
      "  γ = 0.9\n",
      "  ε = 0.1\n",
      "  α = 0.2\n",
      "  Episodes = 5000\n",
      "  Steps = 5000\n",
      "\n",
      "Evaluating optimal value function and policy...\n",
      "\n",
      "Optimal Value Function:\n",
      "21.98 24.42 21.98 19.42 17.48\n",
      "19.78 21.98 19.78 17.80 16.02\n",
      "17.80 19.78 17.80 16.02 14.42\n",
      "16.02 17.80 16.02 14.42 12.98\n",
      "14.42 16.02 14.42 12.98 11.68\n",
      "\n",
      "Optimal Policy:\n",
      "east  south west  west  west \n",
      "north north north west  west \n",
      "north north west  north west \n",
      "east  north north west  north\n",
      "north north west  north north\n",
      "\n",
      "Optimal Policy (arrows):\n",
      "→ ↓ ← ← ←\n",
      "↑ ↑ ↑ ← ←\n",
      "↑ ↑ ← ↑ ←\n",
      "→ ↑ ↑ ← ↑\n",
      "↑ ↑ ← ↑ ↑\n"
     ]
    }
   ],
   "source": [
    "# ——— Setup ———\n",
    "env = GridWorld()\n",
    "states = [(i,j) for i in range(env.height) for j in range(env.width)]\n",
    "qtable = QTable(states, env.actions)\n",
    "\n",
    "# ——— Hyperparameters (exact) ———\n",
    "episodes = 5000\n",
    "steps    = 5000\n",
    "alpha    = 0.2\n",
    "gamma    = 0.9\n",
    "epsilon  = 0.1\n",
    "\n",
    "# ——— Initialization printout ———\n",
    "print(\"Initializing Gridworld...\")\n",
    "print(f\"Grid size: {env.height}x{env.width}\")\n",
    "print(\"Special_states = {'A': (0,1), 'B': (0,3)}\")\n",
    "print(\"Next_to_states = {\\\"A'\\\": (4,1), \\\"B'\\\": (2,3)}\")\n",
    "print(\"Special_rewards = {'A': 10, 'B': 5}\")\n",
    "print(\"Starting Q-learning with parameters:\")\n",
    "print(f\"  γ = {gamma}\")\n",
    "print(f\"  ε = {epsilon}\")\n",
    "print(f\"  α = {alpha}\")\n",
    "print(f\"  Episodes = {episodes}\")\n",
    "print(f\"  Steps = {steps}\\n\")\n",
    "\n",
    "# ——— Training ———\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    for __ in range(steps):\n",
    "        if random.random() < epsilon:\n",
    "            action = random.choice(env.actions)\n",
    "        else:\n",
    "            action = qtable.get_best_action(state)\n",
    "        nxt, reward = env.step(action)\n",
    "        qtable.update(state, action, reward, nxt, alpha, gamma)\n",
    "        state = nxt\n",
    "\n",
    "# ——— Evaluation printout ———\n",
    "print(\"Evaluating optimal value function and policy...\\n\")\n",
    "\n",
    "# Value function\n",
    "V = np.zeros((env.height, env.width))\n",
    "for i in range(env.height):\n",
    "    for j in range(env.width):\n",
    "        V[i,j] = qtable.get_max_q_value((i,j))\n",
    "\n",
    "print(\"Optimal Value Function:\")\n",
    "for i in range(env.height):\n",
    "    print(\" \".join(f\"{V[i,j]:5.2f}\" for j in range(env.width)))\n",
    "\n",
    "# Policy names\n",
    "pi = get_optimal_policy(qtable)\n",
    "print(\"\\nOptimal Policy:\")\n",
    "for i in range(env.height):\n",
    "    print(\" \".join(pi[(i,j)].ljust(5) for j in range(env.width)))\n",
    "\n",
    "# Policy arrows\n",
    "arrow = {'north':'↑','south':'↓','east':'→','west':'←'}\n",
    "print(\"\\nOptimal Policy (arrows):\")\n",
    "for i in range(env.height):\n",
    "    print(\" \".join(arrow[pi[(i,j)]] for j in range(env.width)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
